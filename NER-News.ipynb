{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "from datetime import datetime\n",
    "from gutenberg.textpreparation.methods import replace_regex, remove_regex\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, vstack\n",
    "import keras\n",
    "import numpy as np\n",
    "from gutenberg.classification.keras import mlp\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_corpus(corpus, seed_list, mask_name):\n",
    "    return [replace_regex(doc, regex=[f'\\\\b{e}\\\\b' for e in seed_list], \n",
    "                          replacement=f'\\u0002{mask_name}\\u0002') \n",
    "            for doc in corpus]\n",
    "\n",
    "def get_vectorizer(masked_corpus, mask_name):\n",
    "    vectorizer = TfidfVectorizer(min_df=0.01, max_df=0.5, analyzer='char_wb', ngram_range=(3,5))\n",
    "    vectorizer.fit([remove_regex(doc, regex=f'\\u0002{mask_name}\\u0002') for doc in masked_corpus])\n",
    "    return vectorizer\n",
    "\n",
    "def get_training_data(masked_corpus, mask_name, window, vectorizer):\n",
    "    y_train = []\n",
    "    X_train = None\n",
    "\n",
    "    for document in masked_corpus:\n",
    "        tokenized = document.split()\n",
    "        tokenized_padded = ['']*window + tokenized + ['']*window\n",
    "        for i in range(window,len(tokenized)+window):\n",
    "            pre_vector = vectorizer.transform([' '.join(tokenized_padded[i-window-1:i-1])])\n",
    "            post_vector = vectorizer.transform([' '.join(tokenized_padded[i+1:i+window+1])])\n",
    "            final_vector = hstack((pre_vector, post_vector))\n",
    "            if X_train == None:\n",
    "                X_train = final_vector\n",
    "            else:\n",
    "\n",
    "                X_train = vstack((X_train, final_vector))\n",
    "\n",
    "            if  f'\\u0002{mask_name}\\u0002' in tokenized_padded[i]:\n",
    "                y_train.append(1)\n",
    "                X_train = vstack((X_train, final_vector))\n",
    "            else:\n",
    "                y_train.append(0)\n",
    "                \n",
    "    y_train = np.array(y_train)\n",
    "    y_train = to_categorical(y_train, num_classes=None)\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def train_classifier(X, y):\n",
    "    CLF_PARAMS_DEFINE_MODEL = {\n",
    "        'in_dim': X.shape[1],\n",
    "        'out_dim': y.shape[1],\n",
    "        'use_embedding_layer': False,\n",
    "        'num_hidden_layers': 1,\n",
    "        'num_units': [100],\n",
    "        'use_bias': False,\n",
    "        'use_batch_norm': True,\n",
    "        'activation': 'relu',\n",
    "        'activation_last_layer': 'softmax',\n",
    "        'dropout': 0.5,\n",
    "    }\n",
    "\n",
    "    CLF_PARAMS_COMPILE_MODEL = {\n",
    "        'optimizer': keras.optimizers.Adam(amsgrad=False,\n",
    "                                           beta_1=0.9,\n",
    "                                           beta_2=0.999,\n",
    "                                           decay=0.00,\n",
    "                                           epsilon=1e-8,\n",
    "                                           lr=0.001),\n",
    "        'loss': 'categorical_crossentropy',\n",
    "        'metrics': ['accuracy'],\n",
    "        'num_gpus': 1,\n",
    "    }\n",
    "\n",
    "    CLF_PARAMS_TRAIN = {\n",
    "        'batch_size': 5,\n",
    "        'epochs': 10,\n",
    "        'shuffle': True,\n",
    "        'verbose': 1\n",
    "    }\n",
    "\n",
    "\n",
    "    model = mlp.define_architecture(**CLF_PARAMS_DEFINE_MODEL)\n",
    "\n",
    "    clf = KerasClassifier(build_fn=mlp.compile_model,\n",
    "                               model=model,\n",
    "                               **CLF_PARAMS_COMPILE_MODEL,\n",
    "                               **CLF_PARAMS_TRAIN)\n",
    "    history = clf.fit(x=X.todense(), y=y)\n",
    "    \n",
    "    return clf\n",
    "    \n",
    "def get_name_probabilities(corpus, vectorizer, classifier, window):\n",
    "    name_probas = []\n",
    "    for document in corpus:\n",
    "        vecs = None\n",
    "        tokenized = document.split()\n",
    "        tokenized_padded = ['']*window + tokenized + ['']*window\n",
    "        for i in range(window,len(tokenized)+window):\n",
    "            pre_vector = vectorizer.transform(tokenized_padded[i-window:i-1])\n",
    "            post_vector = vectorizer.transform(tokenized_padded[i+1:i+window])\n",
    "            stacked_vec = hstack((pre_vector, post_vector))\n",
    "            if vecs == None:\n",
    "                vecs = stacked_vec\n",
    "            else:\n",
    "                vecs = vstack((vecs, stacked_vec))\n",
    "        name_probas.append(list(zip(tokenized, classifier.predict_proba(vecs.todense())[:,1])))\n",
    "    return name_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('deu_news_2015_1M-words.txt', 'r') as f:\n",
    "    corpus = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "65/65 [==============================] - 2s 37ms/step - loss: 0.7356 - acc: 0.5846\n",
      "Epoch 2/10\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.4317 - acc: 0.8462\n",
      "Epoch 3/10\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.3052 - acc: 0.9231\n",
      "Epoch 4/10\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.2003 - acc: 0.9692\n",
      "Epoch 5/10\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.2127 - acc: 0.9385\n",
      "Epoch 6/10\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.2108 - acc: 0.9231\n",
      "Epoch 7/10\n",
      "65/65 [==============================] - 0s 7ms/step - loss: 0.1814 - acc: 0.8923\n",
      "Epoch 8/10\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.1998 - acc: 0.9231\n",
      "Epoch 9/10\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.1282 - acc: 0.9692\n",
      "Epoch 10/10\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.1439 - acc: 0.9692\n",
      "14/14 [==============================] - 1s 42ms/step\n",
      "13/13 [==============================] - 0s 3ms/step\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# List of Strings\n",
    "CORPUS = ['Hallo liebes Otto Team, ich heiße Olga Fischer. Ich bin erbost! VG O. Fischer',\n",
    "          'Ich würde gerne meine Adresse ändern.  Aber wie nur? liebe Grüße, Olga Schulz',\n",
    "          'Ich heiße Marius - warum genau weiß nicht nicht. Alles Gute Marius Fischer',\n",
    "          'blablabla Ich heiße Anette. Wer hätte das gedacht? hahaha VG Anette Bukowski <html//...>']\n",
    "\n",
    "ENTITY = 'NAME'\n",
    "WINDOW_SIZE = 2\n",
    "SEED_LIST = ['O.', 'Fischer', 'Anette', 'Bukowski']\n",
    "\n",
    "mc = mask_corpus(CORPUS, SEED_LIST, ENTITY)\n",
    "vec = get_vectorizer(mc, ENTITY)\n",
    "X, y = get_training_data(mc, ENTITY, WINDOW_SIZE, vec)\n",
    "clf = train_classifier(X, y)\n",
    "name_probas = get_name_probabilities(CORPUS, vec, clf, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Bukowski', 0.93043846), ('O.', 0.9128145), ('Ich', 0.9099202), ('Fischer', 0.874008), ('Anette', 0.8027109), ('heiße', 0.79287505), ('Marius', 0.751707), ('VG', 0.6666137), ('Wer', 0.65651304), ('ich', 0.16999942), ('Fischer.', 0.15111297), ('Ich', 0.15111297), ('blablabla', 0.15111297), ('Ich', 0.15111297), ('Marius', 0.1268803), ('Ich', 0.10157568), ('Fischer', 0.09750612), ('genau', 0.086602814), ('Olga', 0.075252056), ('Alles', 0.062577605), ('erbost!', 0.05647908), ('Anette.', 0.038898602), ('würde', 0.031528313), ('Hallo', 0.028852971), ('Team,', 0.025913384), ('Schulz', 0.023723705), ('nur?', 0.023605812), ('nicht', 0.023324596), ('-', 0.02200066), ('liebes', 0.02177464), ('Grüße,', 0.01609526), ('bin', 0.015925078), ('<html//...>', 0.015608985), ('heiße', 0.013326765), ('hätte', 0.010833359), ('liebe', 0.010016993), ('Aber', 0.008710182), ('nicht.', 0.0086892415), ('wie', 0.008491383), ('hahaha', 0.008173939), ('warum', 0.0071243225), ('heiße', 0.006926505), ('Olga', 0.0054117483), ('gerne', 0.0049284445), ('ändern.', 0.0043480345), ('weiß', 0.0041833944), ('Otto', 0.003408874), ('VG', 0.0032127486), ('Gute', 0.0024437122), ('gedacht?', 0.00198396), ('das', 0.0017487336), ('Adresse', 0.0014203584), ('meine', 0.0012822082)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted([x for sublist in name_probas for x in sublist], key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "71/71 [==============================] - 2s 34ms/step - loss: 0.6881 - acc: 0.6901\n",
      "Epoch 2/10\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 0.4177 - acc: 0.8169\n",
      "Epoch 3/10\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.3656 - acc: 0.8310\n",
      "Epoch 4/10\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.2909 - acc: 0.9014\n",
      "Epoch 5/10\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.1707 - acc: 0.9437\n",
      "Epoch 6/10\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.1011 - acc: 0.9718\n",
      "Epoch 7/10\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 0.1232 - acc: 0.9577\n",
      "Epoch 8/10\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 0.1623 - acc: 0.9577\n",
      "Epoch 9/10\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 0.2141 - acc: 0.9296\n",
      "Epoch 10/10\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 0.1318 - acc: 0.9577\n",
      "14/14 [==============================] - 1s 42ms/step\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "13/13 [==============================] - 0s 1ms/step\n",
      "13/13 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "SEED_LIST = ['O', 'Fischer', 'Anette', 'Bukowski', 'Marius']\n",
    "\n",
    "mc = mask_corpus(CORPUS, SEED_LIST, ENTITY)\n",
    "vec = get_vectorizer(mc, ENTITY)\n",
    "X, y = get_training_data(mc, ENTITY, WINDOW_SIZE, vec)\n",
    "clf = train_classifier(X, y)\n",
    "name_probas = get_name_probabilities(CORPUS, vec, clf, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('O.', 0.996414), ('Marius', 0.99117684), ('-', 0.98489064), ('Bukowski', 0.96321946), ('Ich', 0.90885305), ('heiße', 0.83889604), ('Fischer', 0.8017361), ('Gute', 0.77318066), ('Anette', 0.6641807), ('Wer', 0.3766854), ('Anette.', 0.21449465), ('ich', 0.16215149), ('Fischer.', 0.1426241), ('Ich', 0.1426241), ('heiße', 0.1426241), ('blablabla', 0.1426241), ('Ich', 0.1426241), ('<html//...>', 0.14229205), ('Schulz', 0.07342339), ('hätte', 0.06984317), ('VG', 0.04627905), ('ändern.', 0.044727873), ('Team,', 0.043830343), ('VG', 0.042113457), ('Adresse', 0.039288096), ('nicht', 0.0390156), ('Olga', 0.03743849), ('das', 0.03716049), ('Marius', 0.029196005), ('Ich', 0.022962805), ('liebes', 0.020000061), ('Aber', 0.01814909), ('Fischer', 0.017445108), ('Olga', 0.016784677), ('gerne', 0.014359055), ('weiß', 0.012942421), ('würde', 0.011884689), ('nur?', 0.011348455), ('gedacht?', 0.011232781), ('Grüße,', 0.0096255895), ('Hallo', 0.0076198652), ('erbost!', 0.0045330366), ('warum', 0.0044925674), ('meine', 0.0044485484), ('nicht.', 0.003458289), ('liebe', 0.0032475719), ('genau', 0.0027476675), ('bin', 0.002505851), ('wie', 0.0014410939), ('heiße', 0.0013070817), ('Alles', 0.0012026683), ('Otto', 0.0006045561), ('hahaha', 0.0001874584)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted([x for sublist in name_probas for x in sublist], key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (cicero)",
   "language": "python",
   "name": "cicero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
