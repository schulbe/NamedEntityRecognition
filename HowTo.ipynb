{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-moscow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "intense-digest",
   "metadata": {},
   "source": [
    "# Get your Data\n",
    "Normally a large dataset will be loaded, here we write a small one out for demsontration purposes.\n",
    "Say we have a set of Tweets. Surely they are very constructed but the principle works if the corpus is large enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "clean-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'OMG! I love driving my new Mercedes. It is so fast.',\n",
    "    'Guys see how cool my friend looks driving my new VW. He\\'s loving it!',\n",
    "    'I have always dreamt of buying a campervan from my friend.'\n",
    "    'Today I will finally get my new Computer.'\n",
    "    'I have always dreamt of a campervan from VW.'\n",
    "    'He doesn\\'t seem to like driving my new Lamborghini! Maybe not fast enough :D'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-nudist",
   "metadata": {},
   "source": [
    "# Define a seedlist\n",
    "The goal is to find entities in your corpus without the need of using pretrained models to do so. This makes it more robust to spelling or grammer mistakes (especially in non-english contexts) and also lifts limitations of what an entity might be.\n",
    "\n",
    "In this case we want to identify words IN CONTEXT that might be car brands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "commercial-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = ['mercedes', 'lamborghini']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-denial",
   "metadata": {},
   "source": [
    "# Create Tagger\n",
    "This is the main framework to do the iterative training described in README.md \n",
    "To apply less restrictions (since corpora may be very different in nature) the actual model definition and embedding used will be defined separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "approximate-lloyd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " [==================================================] 100.00% [0:00:00 process time]\n"
     ]
    }
   ],
   "source": [
    "from src.NER import NERTagger\n",
    "\n",
    "tagger = NERTagger(corpus,\n",
    "                   entities = [{'name': 'CAR_BRAND', 'seed': seed_list}],\n",
    "                   seed = 123456, # for reproducability\n",
    "                   window = 3, # context window around desired word ( designed for not using advanced layers like LSTM)\n",
    "                   n_jobs = 1, # for large datasets, multiple jobs will be faster\n",
    "                   train_min_pos_rate = 0.4 # How confident does the model have to be in order to adjust the seed list\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-madrid",
   "metadata": {},
   "source": [
    "# Create Embedding and Model\n",
    "Since this is a Proof of Concept, we will be using very simple word embeddings (basically counts) and neural networks since our corpus is too small for anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "designing-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 20\n",
    "model_dims = tagger.get_required_dimensions()\n",
    "\n",
    "mlp_model = Sequential()\n",
    "mlp_model.add(Embedding(model_dims['num_labels'], EMBEDDING_SIZE, input_length=model_dims['in_dim']))\n",
    "mlp_model.add(Flatten())\n",
    "mlp_model.add(Dense(100, activation='relu'))\n",
    "mlp_model.add(Dropout(0.5))\n",
    "mlp_model.add(Dense(model_dims['out_dim'], activation='softmax'))\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 5,\n",
    "    \"loss\": \"categorical_crossentropy\",\n",
    "    \"metrics\": [\"accuracy\"],\n",
    "    \"optimizer\": Adam(amsgrad=False,\n",
    "                      beta_1=0.9,\n",
    "                      beta_2=0.999,\n",
    "                      decay=0.00,\n",
    "                      epsilon=1e-8,\n",
    "                      lr=0.001),\n",
    "}\n",
    "\n",
    "def compile_model(model, loss, optimizer, metrics):\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=compile_model, model=mlp_model, **MODEL_PARAMS)\n",
    "\n",
    "tagger.set_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-finnish",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Now the model is trained in multiple iterations (see main README.md for details) to learn contextualo rules for where in a sentence a car brand might be situated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "developed-joseph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================================] 100.00% [0:00:00 process time]\n",
      "Epoch 1/10\n",
      "14/14 [==============================] - 1s 2ms/step - loss: 1.2803 - accuracy: 0.5913\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.4462 - accuracy: 0.9482\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.8462 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.0114 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.3265 - accuracy: 0.9902\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7994 - accuracy: 0.9933\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.9609 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6315 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5565 - accuracy: 0.9933\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7534 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjorn/.virtualenv/main/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================================] 100.00% [0:00:00 process time]\n",
      " [==================================================] 100.00% [0:00:00 process time]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "generate_config = {\n",
    "    'max_iterations': 5,\n",
    "    'min_probability': 0.3,\n",
    "    'min_update_rate': 0.02\n",
    "}\n",
    "\n",
    "os.makedirs('example_run', exist_ok=True)\n",
    "    \n",
    "tagger.generate_predictive_rules(iteration_save_path='example_run',\n",
    "                                  save_iterations=list(range(generate_config['max_iterations']+1)),\n",
    "                                  **generate_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-superior",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "Now the trained model is evaluated. Performance can obviously not be expected to be brilliant given the minimal dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "heavy-assault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('guys', 0.02955764), ('see', 0.03723589), ('how', 0.061579213), ('cool', 0.1061618), ('my', 0.17758377), ('friend', 0.18183373), ('looks', 0.29528335), ('driving', 0.14679635), ('my', 0.14607784), ('new', 0.17787132), ('vw', 0.49876878), (\"he's\", 0.088515826), ('loving', 0.061647322), ('it', 0.025124522)]\n"
     ]
    }
   ],
   "source": [
    "# Get the probability of every word in the sentence to be of the trained entity. \n",
    "# For simplicity, all words were lower cased and punctuation was reduced to spaces.\n",
    "\n",
    "test_sentence = 'Guys see how cool my friend looks driving my new VW. He\\'s loving it!'\n",
    "probabs = tagger.predict_token_probabilities(test_sentence)\n",
    "print(probabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "meaning-nebraska",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in sentence that might be car brands:\n",
      "\n",
      "vw (Token number 10)\n"
     ]
    }
   ],
   "source": [
    "# Now only print a sentences words that has a contextual likelihood > cutoff for being of the given entity.\n",
    "\n",
    "cutoff = 0.5   # normally much higher for appropriately sized data sets\n",
    "\n",
    "brands = [(p[0], ix) for ix, p in enumerate(probabs) if round(p[1],2)>=cutoff]\n",
    "\n",
    "print('Words in sentence that are likely car brands:\\n')\n",
    "for b in brands:\n",
    "    print(f'{b[0]} (Token number {b[1]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "copyrighted-graphic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in sentence that are likely car brands:\n",
      "\n",
      "unknowncarbrand (Token number 10)\n"
     ]
    }
   ],
   "source": [
    "# Notice that this is ONLY CONTEXTUAL and not based on a word list -> Brand does not need to be known in advance\n",
    "# Although the tagger is more designed to tag the corpus it is trained on, it would theoretically also work on \n",
    "# unknown words \n",
    "\n",
    "test_sentence = 'Guys see how cool my friend looks driving my new UnknownCarBrand. He\\'s loving it!'\n",
    "\n",
    "probabs = tagger.predict_token_probabilities(test_sentence)\n",
    "brands = [(p[0], ix) for ix, p in enumerate(probabs) if round(p[1],2)>=cutoff]\n",
    "\n",
    "print('Words in sentence that are likely car brands:\\n')\n",
    "for b in brands:\n",
    "    print(f'{b[0]} (Token number {b[1]})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-excitement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
