{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-moscow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "intense-digest",
   "metadata": {},
   "source": [
    "# Get your Data\n",
    "Normally a large dataset will be loaded, here we write a small one out for demsontration purposes.\n",
    "Say we have a set of Tweets. Surely they are very constructed but the principle works if the corpus is large enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "clean-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'OMG! I love driving my new Mercedes. It is so fast.',\n",
    "    'Guys see how cool my friend looks driving my new VW. He\\'s loving it!',\n",
    "    'I have always dreamt of buying a campervan from my friend.'\n",
    "    'Today I will finally get my new Computer.'\n",
    "    'I have always dreamt of a campervan from VW.'\n",
    "    'He doesn\\'t seem to like driving my new Lamborghini! Maybe not fast enough :D'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-nudist",
   "metadata": {},
   "source": [
    "# Define a seedlist\n",
    "The goal is to find entities in your corpus without the need of using pretrained models to do so. This makes it more robust to spelling or grammer mistakes (especially in non-english contexts) and also lifts limitations of what an entity might be.\n",
    "\n",
    "In this case we want to identify words IN CONTEXT that might be car brands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "commercial-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = ['mercedes', 'lamborghini']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-denial",
   "metadata": {},
   "source": [
    "# Create Tagger\n",
    "This is the main framework to do the iterative training described in README.md \n",
    "To apply less restrictions (since corpora may be very different in nature) the actual model definition and embedding used will be defined separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "approximate-lloyd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " [==================================================] 100.00% [0:00:00 process time]\n"
     ]
    }
   ],
   "source": [
    "from src.NER import NERTagger\n",
    "\n",
    "tagger = NERTagger(corpus,\n",
    "                   entities = [{'name': 'CAR_BRAND', 'seed': seed_list}],\n",
    "                   seed = 1234, # for reproducability\n",
    "                   window = 3, # context window around desired word ( designed for not using advanced layers like LSTM)\n",
    "                   n_jobs = 1, # for large datasets, multiple jobs will be faster\n",
    "                   train_min_pos_rate = 0.5 # How confident does the model have to be in order to adjust the seed list\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-madrid",
   "metadata": {},
   "source": [
    "# Create Embedding and Model\n",
    "Since this is a Proof of Concept, we will be using very simple word embeddings (basically counts) and neural networks since our corpus is too small for anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "designing-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 100\n",
    "model_dims = tagger.get_required_dimensions()\n",
    "\n",
    "mlp_model = Sequential()\n",
    "mlp_model.add(Embedding(model_dims['num_labels'], EMBEDDING_SIZE, input_length=model_dims['in_dim']))\n",
    "mlp_model.add(Flatten())\n",
    "mlp_model.add(Dense(20, activation='relu'))\n",
    "mlp_model.add(Dropout(0.5))\n",
    "mlp_model.add(Dense(model_dims['out_dim'], activation='softmax'))\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 5,\n",
    "    \"loss\": \"categorical_crossentropy\",\n",
    "    \"metrics\": [\"accuracy\"],\n",
    "    \"optimizer\": Adam(amsgrad=False,\n",
    "                      beta_1=0.9,\n",
    "                      beta_2=0.999,\n",
    "                      decay=0.00,\n",
    "                      epsilon=1e-8,\n",
    "                      lr=0.01),\n",
    "}\n",
    "\n",
    "def compile_model(model, loss, optimizer, metrics):\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "mlp_model.compile(loss= \"categorical_crossentropy\",\n",
    "    metrics= [\"accuracy\"],\n",
    "    optimizer= Adam(amsgrad=False,\n",
    "                      beta_1=0.9,\n",
    "                      beta_2=0.999,\n",
    "                      decay=0.00,\n",
    "                      epsilon=1e-8,\n",
    "                      lr=0.01))\n",
    "model = KerasClassifier(build_fn=compile_model, model=mlp_model, **MODEL_PARAMS)\n",
    "\n",
    "tagger.set_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-finnish",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "developed-joseph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================================] 100.00% [0:00:00 process time]\n",
      "Epoch 1/10\n",
      "14/14 [==============================] - 1s 3ms/step - loss: 1.3874 - accuracy: 0.6770\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3063 - accuracy: 0.9861\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.1080 - accuracy: 0.9801\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 8.9770e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.9300e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      " [==================================================] 100.00% [0:00:00 process time]\n",
      " [==================================================] 100.00% [0:00:00 process time]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "generate_config = {\n",
    "    'max_iterations': 20,\n",
    "    'min_probability': 0.3,\n",
    "    'min_update_rate': 0.02\n",
    "}\n",
    "\n",
    "os.makedirs('example_run', exist_ok=True)\n",
    "    \n",
    "tagger.generate_predictive_rules(iteration_save_path='example_run',\n",
    "                                  save_iterations=list(range(generate_config['max_iterations']+1)),\n",
    "                                  **generate_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "heavy-assault",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guys', 6.7550886e-06),\n",
       " ('see', 1.04563114e-07),\n",
       " ('how', 2.1449303e-08),\n",
       " ('cool', 2.4688078e-09),\n",
       " ('my', 1.4934152e-08),\n",
       " ('friend', 1.0560224e-05),\n",
       " ('looks', 5.091005e-06),\n",
       " ('driving', 2.5658253e-05),\n",
       " ('my', 3.4715442e-06),\n",
       " ('new', 0.0009779446),\n",
       " ('vw', 0.0065938407),\n",
       " (\"he's\", 3.4171895e-05),\n",
       " ('loving', 7.38642e-05),\n",
       " ('it', 1.0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.predict_token_probabilities('Guys see how cool my friend looks driving my new VW. He\\'s loving it!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-nebraska",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
