                {
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from datetime import datetime\n",
    "from gutenberg.textpreparation.methods import replace_regex, remove_regex\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, vstack\n",
    "import keras\n",
    "import numpy as np\n",
    "from gutenberg.classification.keras import mlp\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import to_categorical\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_count(corpus):\n",
    "    token_count = defaultdict(lambda: 0)\n",
    "    overall_tokens = 0\n",
    "    for doc in corpus:\n",
    "        for token in re.split('[^\\w]+', doc):\n",
    "            overall_tokens += 1\n",
    "            token_count[token.lower()]+=1\n",
    "    \n",
    "    return {k: v/overall_tokens for k, v in token_count.items()} \n",
    "\n",
    "def mask_corpus(corpus, seed_list, mask_name):\n",
    "    return [replace_regex(doc, regex=[f'\\\\b{e}\\\\b' for e in seed_list], \n",
    "                          replacement=f'\\u0002{mask_name}\\u0002', ignore_case=True) \n",
    "            for doc in corpus]\n",
    "\n",
    "def get_vectorizer(masked_corpus, mask_name):\n",
    "    vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,4), max_features=10000)\n",
    "    vectorizer.fit(masked_corpus)\n",
    "    return vectorizer\n",
    "\n",
    "def get_training_data(masked_corpus, mask_name, window, vectorizer):\n",
    "    y_train = []\n",
    "    X_train = None\n",
    "    for doc_num, document in enumerate(masked_corpus):\n",
    "        if (doc_num+1)%50 == 0:\n",
    "            print(f'processed {doc_num+1}/{len(masked_corpus)}')\n",
    "        tokenized = re.split('[^\\w\\u0002]+', document)\n",
    "        tokenized_padded = ['']*window + tokenized + ['']*window\n",
    "        for i, token in enumerate(tokenized):\n",
    "            pre_vector = vectorizer.transform([' '.join(tokenized_padded[i:i+window])])\n",
    "            post_vector = vectorizer.transform([' '.join(tokenized_padded[i+window+1:i+2*window+1])])\n",
    "            final_vector = hstack((pre_vector, post_vector))\n",
    "            if X_train == None:\n",
    "                X_train = final_vector\n",
    "            else:\n",
    "\n",
    "                X_train = vstack((X_train, final_vector))\n",
    "\n",
    "            if  f'\\u0002{mask_name}\\u0002' in token:\n",
    "                y_train.append(1)\n",
    "            else:\n",
    "                y_train.append(0)\n",
    "                \n",
    "    y_train = np.array(y_train)\n",
    "    y_train = to_categorical(y_train, num_classes=None)\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def setup(GPU_ID=None, clear_session=False):\n",
    "    \"\"\"Classification setup\n",
    "\n",
    "    :param config: configuration\n",
    "    :param clear_session: flag whether Tensorflow session is cleaned\n",
    "    \"\"\"\n",
    "    if clear_session:\n",
    "        K.clear_session()\n",
    "\n",
    "    if GPU_ID is not None:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = GPU_ID\n",
    "        runtime_classifier_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        runtime_classifier_config.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(config=runtime_classifier_config)\n",
    "    else:\n",
    "        os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "        runtime_classifier_config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "        sess = tf.Session(config=runtime_classifier_config)\n",
    "\n",
    "    K.set_session(sess)\n",
    "\n",
    "    \n",
    "def train_classifier(X, y):\n",
    "    CLF_PARAMS_DEFINE_MODEL = {\n",
    "        'in_dim': X.shape[1],\n",
    "        'out_dim': y.shape[1],\n",
    "        'use_embedding_layer': False,\n",
    "        'num_hidden_layers': 1,\n",
    "        'num_units': [100],\n",
    "        'use_bias': False,\n",
    "        'use_batch_norm': True,\n",
    "        'activation': 'relu',\n",
    "        'activation_last_layer': 'softmax',\n",
    "        'dropout': 0.5,\n",
    "    }\n",
    "\n",
    "    CLF_PARAMS_COMPILE_MODEL = {\n",
    "        'optimizer': keras.optimizers.Adam(amsgrad=False,\n",
    "                                           beta_1=0.9,\n",
    "                                           beta_2=0.999,\n",
    "                                           decay=0.00,\n",
    "                                           epsilon=1e-8,\n",
    "                                           lr=0.001),\n",
    "        'loss': 'categorical_crossentropy',\n",
    "        'metrics': ['accuracy'],\n",
    "        'num_gpus': 1,\n",
    "    }\n",
    "\n",
    "    CLF_PARAMS_TRAIN = {\n",
    "        'batch_size': 5,\n",
    "        'epochs': 10,\n",
    "        'shuffle': True,\n",
    "        'verbose': 1\n",
    "    }\n",
    "\n",
    "\n",
    "    model = mlp.define_architecture(**CLF_PARAMS_DEFINE_MODEL)\n",
    "\n",
    "    clf = KerasClassifier(build_fn=mlp.compile_model,\n",
    "                               model=model,\n",
    "                               **CLF_PARAMS_COMPILE_MODEL,\n",
    "                               **CLF_PARAMS_TRAIN)\n",
    "    history = clf.fit(x=X.todense(), y=y)\n",
    "    \n",
    "    return clf\n",
    "    \n",
    "def get_name_probabilities(corpus, vectorizer, classifier, window, token_count, thres):\n",
    "    name_probas = []\n",
    "    for document in corpus:\n",
    "        vecs = None\n",
    "        tokenized = re.split('[^\\w\\u0002]+', document)\n",
    "        tokenized_padded = ['']*window + tokenized + ['']*window\n",
    "        for i, token in enumerate(tokenized):\n",
    "            pre_vector = vectorizer.transform([' '.join(tokenized_padded[i:i+window])])\n",
    "            post_vector = vectorizer.transform([' '.join(tokenized_padded[i+window+1:i+2*window+1])])\n",
    "            stacked_vec = hstack((pre_vector, post_vector))\n",
    "            if vecs == None:\n",
    "                vecs = stacked_vec\n",
    "            else:\n",
    "                vecs = vstack((vecs, stacked_vec))\n",
    "        name_probas.append(list(zip(tokenized, classifier.predict_proba(vecs.todense())[:,1])))\n",
    "    return name_probas\n",
    "\n",
    "def duplicate_positives(X, y):\n",
    "    new_positive_idx = np.random.choice(np.argwhere(y[:,1]==1).reshape(-1,),\n",
    "                                        int(y.shape[0] - np.sum(y[:,1])))\n",
    "    X = X.tocsr()\n",
    "    X = vstack((X, X[new_positive_idx,:]))\n",
    "    \n",
    "    y = np.append(y, np.array(len(new_positive_idx)*[[0,1]]).reshape(-1,2), axis=0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1902, 2)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.000074 -- Mask Corpus...\n",
      "0:00:00.000814 -- Create Vectorizer...\n",
      "0:00:00.005557 -- Generate Train & Test...\n",
      "0:00:00.173297 -- Duplicate Positives...\n",
      "0:00:00.174954 -- Train Classifier...\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.7286 - acc: 0.5800\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.3671 - acc: 0.8800\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.2335 - acc: 0.9000\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1707 - acc: 0.9500\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1084 - acc: 0.9800\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 0.1161 - acc: 0.9600\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.1356 - acc: 0.9400\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.1443 - acc: 0.9800\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0819 - acc: 0.9800\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0501 - acc: 0.9900\n",
      "0:00:08.507753 -- Calculate Name Probabilities...\n",
      "14/14 [==============================] - 0s 29ms/step\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "14/14 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# List of Strings\n",
    "CORPUS = ['Hallo liebes Otto Team, ich heiße Olga Fischer. Ich bin erbost! VG O. Fischer',\n",
    "          'Ich würde gerne meine Adresse ändern.  Aber wie nur? liebe Grüße, Olga Schulz',\n",
    "          'Ich heiße Marius - warum genau weiß nicht nicht. Alles Gute Marius Fischer',\n",
    "          'blablabla Ich heiße Anette. Wer hätte das gedacht? hahaha VG Anette Bukowski <html//...>']\n",
    "\n",
    "ENTITY = 'NAME'\n",
    "WINDOW_SIZE = 2\n",
    "SEED_LIST = ['O.', 'Fischer', 'Olga', 'Schulz']\n",
    "\n",
    "token_rel = get_token_count(CORPUS)\n",
    "\n",
    "t = datetime.now()\n",
    "print(f'{(datetime.now()-t)} -- Mask Corpus...') \n",
    "mc = mask_corpus(CORPUS, SEED_LIST, ENTITY)                         # t = 0:01 m\n",
    "print(f'{(datetime.now()-t)} -- Create Vectorizer...')\n",
    "vec = get_vectorizer(mc, ENTITY)                                    # t = 0:00,16 m\n",
    "print(f'{(datetime.now()-t)} -- Generate Train & Test...')\n",
    "X, y = get_training_data(mc, ENTITY, WINDOW_SIZE, vec)              # t = 07:00 m \n",
    "print(f'{(datetime.now()-t)} -- Duplicate Positives...')\n",
    "X, y = duplicate_positives(X, y)\n",
    "print(f'{(datetime.now()-t)} -- Train Classifier...')\n",
    "clf = train_classifier(X, y)\n",
    "print(f'{(datetime.now()-t)} -- Calculate Name Probabilities...')\n",
    "name_probas = get_name_probabilities(CORPUS, vec, clf, WINDOW_SIZE,token_rel, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Grüße', 0.037077684), ('warum', 0.015481879), ('Anette', 0.014906197), ('blablabla', 0.013013004), ('O', 0.011844105), ('Ich', 0.0116691515), ('heiße', 0.008858857), ('heiße', 0.0071677812), ('', 0.006684412), ('Alles', 0.0052263397), ('gerne', 0.004716766), ('Ich', 0.004690479), ('Marius', 0.004404278), ('Ich', 0.0037930894), ('Marius', 0.0037120483), ('würde', 0.003395978), ('Hallo', 0.003060839), ('Ich', 0.0024337696), ('heiße', 0.0022659018), ('erbost', 0.0020008103), ('html', 0.0016377432), ('meine', 0.0015781426), ('genau', 0.0015290531), ('Team', 0.0013206884), ('Adresse', 0.0011763335), ('Otto', 0.0010172601), ('Wer', 0.0007112263), ('hätte', 0.0007025649), ('VG', 0.0006440501), ('Anette', 0.00048776015), ('nicht', 0.00047979638), ('Gute', 0.0004489051), ('nur', 0.00044219548), ('Bukowski', 0.00039082346), ('bin', 0.000336014), ('das', 0.0003244043), ('liebe', 0.00030454333), ('wie', 0.0002861562), ('ändern', 0.00023489875), ('gedacht', 0.00022169587), ('liebes', 0.00017910257), ('weiß', 0.00016259355), ('Aber', 0.00013455402), ('ich', 0.00012768891), ('nicht', 0.000118118274), ('hahaha', 0.00011299277), ('VG', 4.940936e-05)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted([x for sublist in name_probas for x in sublist if not x[0] in SEED_LIST], key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Olga', 0.861947), ('VG', 0.82714397), ('Marius', 0.52457404), ('Wer', 0.43621957), ('html', 0.37051007), ('heiße', 0.32507634), ('O', 0.09527617), ('Ich', 0.065555796), ('hahaha', 0.024592336), ('', 0.021146769), ('blablabla', 0.019551756), ('Ich', 0.018869512), ('hätte', 0.01646993), ('gerne', 0.016128035), ('das', 0.014390469), ('Schulz', 0.013798401), ('Gute', 0.012183242), ('Ich', 0.010419562), ('gedacht', 0.008621378), ('ändern', 0.0068459725), ('Alles', 0.0062777954), ('Otto', 0.0062291925), ('bin', 0.004623598), ('nicht', 0.004613282), ('heiße', 0.003694078), ('VG', 0.0034506102), ('Grüße', 0.0034505085), ('Marius', 0.0032813202), ('Ich', 0.0030251488), ('genau', 0.0028159316), ('heiße', 0.0026944727), ('Hallo', 0.0026158604), ('weiß', 0.002189594), ('nicht', 0.001906073), ('Adresse', 0.0016501271), ('erbost', 0.0015771948), ('Team', 0.0015620489), ('liebes', 0.0015105457), ('meine', 0.0014741501), ('würde', 0.0014587694), ('Aber', 0.0012878149), ('warum', 0.00092750334), ('liebe', 0.0008275538), ('Olga', 0.0007339487), ('ich', 0.00061293767), ('wie', 0.00016505709), ('nur', 2.4327926e-05)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted([x for sublist in name_probas for x in sublist if not x[0] in SEED_LIST], key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (cicero)",
   "language": "python",
   "name": "cicero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
